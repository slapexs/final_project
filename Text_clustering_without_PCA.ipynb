{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ref: https://youtu.be/H9OmsD7F7p0"
      ],
      "metadata": {
        "id": "G5YxaAzgl-6c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWZivjELl8Lr"
      },
      "outputs": [],
      "source": [
        "!pip install pythainlp\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4Wdogr2mE_9",
        "outputId": "fd8a5048-e35b-4f6a-e948-b9f9d130da23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = '/content/drive/MyDrive/Final_project/clean_data/combine_company.csv'"
      ],
      "metadata": {
        "id": "X7gCDuJImGdo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pythainlp.corpus import thai_stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "SJIA1idDmJAx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(data)\n",
        "frozen_stopword = thai_stopwords()"
      ],
      "metadata": {
        "id": "0IsQVMHhmKUK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean stop word and special charater\n",
        "import string\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "english_stopword = stopwords.words('english')\n",
        "thai_stopword = list(frozen_stopword)\n",
        "\n",
        "def clean_tokenize(rec_text:str) -> list:\n",
        "  clean_token = rec_text\n",
        "  for pair in (('\\n', ''), ('\\t', ''), ('!', '')):\n",
        "        clean_token =clean_token.replace(*pair)\n",
        "  \n",
        "  temp_tokenize = word_tokenize(clean_token, None, 'newmm', False)\n",
        "  return temp_tokenize.lower()\n",
        "\n",
        "def clean_stopword(word:list):\n",
        "  temp_thai = []\n",
        "  temp_eng = []\n",
        "  for i in word:\n",
        "    if i not in thai_stopword:\n",
        "      temp_thai.append(i)\n",
        "  \n",
        "  for j in temp_thai:\n",
        "    if j not in english_stopword:\n",
        "      temp_eng.append(j)\n",
        "    \n",
        "  return temp_eng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4jbHTA_mLpe",
        "outputId": "1584b5dd-a19a-4065-9b7e-125592de3494"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List company detail\n",
        "\"\"\"\n",
        "  ตัดรายละเอียดธุรกิจเป็นคำๆ แล้วใส่ในตัวแปร list_company_detail\n",
        "\"\"\"\n",
        "\n",
        "list_company_detail = []\n",
        "for i in range(len(df)):\n",
        "  a = df.iloc[i]['รายละเอียดธุรกิจ'].translate(str.maketrans('','', string.punctuation))\n",
        "  b = a.translate(str.maketrans('','', string.digits))\n",
        "  temp_comp = clean_tokenize(b)\n",
        "  stopword_cleaned = clean_stopword(temp_comp)\n",
        "  list_company_detail.append(stopword_cleaned)"
      ],
      "metadata": {
        "id": "RuyppRwxmNcG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# หาค่า TF-IDF"
      ],
      "metadata": {
        "id": "WCGEC158mP5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "XziG_KA5mS_b"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_fun(text):\n",
        "    return text"
      ],
      "metadata": {
        "id": "goyHaUaanpRc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_df=0.5,\n",
        "    max_features=6000,\n",
        "    min_df=2,\n",
        "    use_idf=True,\n",
        "    stop_words='english',\n",
        "    analyzer='word',\n",
        "    tokenizer=identity_fun,\n",
        "    preprocessor=identity_fun,\n",
        "    token_pattern=None\n",
        ")\n",
        "\n",
        "X = tfidf_vectorizer.fit_transform(list_company_detail)\n",
        "# tfidf_array = np.array(tfidf_vector.todense())\n",
        "\n",
        "#แปลงเป็น DataFrame เพื่อง่ายแก่การอ่าน\n",
        "# df1 = pd.DataFrame(tfidf_array,columns=tfidf_vectorizer.get_feature_names_out())\n",
        "# df1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l2mqyasmWdq",
        "outputId": "304bb028-2f35-4dba-c32e-b962d04379c8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KMeans clustering"
      ],
      "metadata": {
        "id": "Cr9uBaSkmb3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "He_yRIiomduR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km = KMeans(n_clusters=8)"
      ],
      "metadata": {
        "id": "oCBrnmVQmfBj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km.fit(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1xQzZRHo9K4",
        "outputId": "1e0d0f00-9064-49f7-d466-d128a4341bbe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans()"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]"
      ],
      "metadata": {
        "id": "eauWXfCPpDz8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centroid = km.cluster_centers_\n",
        "cluster = km.labels_"
      ],
      "metadata": {
        "id": "K0ncDDZYpR_6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cluster'] = cluster"
      ],
      "metadata": {
        "id": "j5m7ZFfXqrZa"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cluster_map = {\n",
        "    0: \"e-commerce\",\n",
        "    1: \"Chatbot\",\n",
        "    # 1: \"Robotics\",\n",
        "    2: \"Software Development\",\n",
        "    3: \"Artificial Intelligence\",\n",
        "    4: \"Mobile\",\n",
        "    5: \"Internet of Things\",\n",
        "    6: \"Biometrics\",\n",
        "    7: \"Data Science\"\n",
        "}\n",
        "# Embedded System\n",
        "\n",
        "# apply mapping\n",
        "df['cluster'] = df['cluster'].map(cluster_map)"
      ],
      "metadata": {
        "id": "1t_R6PEZqK3o"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "for i in range(5):\n",
        "  ind_sample = random.randint(0, len(df))\n",
        "  print(f'{df.iloc[ind_sample][\"ชื่อย่อสถานประกอบการ\"]}')\n",
        "  print(df.iloc[ind_sample]['รายละเอียดธุรกิจ'])\n",
        "  print(f'cluster label: {cluster[ind_sample]}')\n",
        "  print(f\"Category: {df.iloc[ind_sample]['cluster']}\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVR55tgMsLWp",
        "outputId": "3c0fb9a1-9d1b-47f6-8b81-646a362bacbc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BONZEN\n",
            "บริการออกแบบพัฒนาจัดหาระบบเทคโนโลยีสารสนเทศ IT Solution, Software Development Online Platform Web/Mobile Application, Building and Facility management, Energy Data Management, Engineering data management, construction data management,\n",
            "cluster label: 2\n",
            "Category: Software Development\n",
            "\n",
            "Feedback180\n",
            "บริการงานออกแบบและพัฒนาซอฟต์แวร์อัจฉริยะ (AI Technology) สำหรับวิเคราะห์ข้อมูลทั้งในรูปแบบถ้อยคำและภาพเคลื่อนไหว, บริการจัดหา Solution - Customer VoC360 Insights / Customer Closed Loop Feedback /VDO Analytics Shopper Experience โดยใช้เทคโนโลยี NLP (Natural Language Processing) บวกกับ Machine Learning และ Deep Learning ร่วมกับความรู้ทางด้านภาษาศาสตร์ของทีมงานนักภาษาศาสตร์ (Linguist) และ Data Science ในการวิเคราะห์ “ภาษาธรรมชาติ” หรือภาษาที่เราใช้จริงในชีวิตประจำวัน (ไม่ใช่แค่วิเคราะห์ภาษาตามหลักภาษาไทยในตำราเรียนทั่วไป) ซึ่งเราเก็บรวบรวมเสียงสะท้อนของลูกค้าจากทุกช่องทางทั้ง Online และ Offline แล้วมาสร้างโมเดลที่ช่วยค้นหา Customer Experience,\n",
            "cluster label: 0\n",
            "Category: e-commerce\n",
            "\n",
            "aris\n",
            "บริการโซลูชันการค้าปลีกออนไลน์อัตโนมัติด้วย AI ChatBot และ Live Stream บน Facebook LIVE\n",
            "cluster label: 5\n",
            "Category: Internet of Things\n",
            "\n",
            "Xcellence\n",
            "บริการพัฒนาและจัดจำหน่ายซอฟต์แวร์ระบบวางแผนบริหารจัดการทรัพยากรในองค์กร ERP (Enterprise Resources Planning) และการให้คำปรึกษาด้านไอทีและวางระบบ ภายใต้ชื่อ \"Xcellence ERP\" ซึ่งรองรับธุรกิจได้หลากหลายกลุ่มธุรกิจ เช่น ธุรกิจซื้อมา-ขายไป, ธุรกิจค้าปลีก-ค้าส่ง, ธุรกิจอาหารและเครื่องดื่ม, อุตสาหกรรมการผลิต, อุตสาหกรรมเหล็ก, อุตสาหกรรมพลาสติก, ธุรกิจแฟชั่น เช่น เสื้อผ้า, รองเท้า ตลอดจนธุรกิจลิสซิ่ง เป็นต้น, Cloud Xcellence ERP รูปแบบการประมวลผลแบบ Cloud computing ได้ถูกนำมาใช้ในโปรแกรม ERP\n",
            "cluster label: 6\n",
            "Category: Biometrics\n",
            "\n",
            "Divertise Asia\n",
            "บริการจัดทำโปรแกรมเว็บเพจและเครือข่ายตามวัตถุประสงค์ผู้ใช้, APPLICATION AND PLATFORMS, ARTIFICIAL INTELLIGENCE, WEBSITES, VIRTUAL REALITY, GAMES)\n",
            "cluster label: 1\n",
            "Category: Chatbot\n",
            "\n"
          ]
        }
      ]
    }
  ]
}